---
title: "QPAD offsets"
author: "Peter Solymos <solymos@ualberta.ca>"
---

---
title: "A Primer in Regression Techniques"
author: "Peter Solymos <solymos@ualberta.ca>"
---

> All models are wrong, but some are useful -- Box

## Introduction

This chapter will provide all the foundations we need for the coming chapters. It is not intended as a general and all-exhaustive introduction to regression techniques, but rather the minimum requirement moving forwards. We will also hone our data processing and plotting skills.

## Prerequisites

```{r message=TRUE,warning=FALSE}
library(mefa4)                # data manipulation
library(detect)
source("R/functions.R")         # some useful stuff
load("data/josm-data.rda") # JOSM data
```

```{r}

x <- josm$surveys
idx <- sample(rownames(x), size = 500, replace = F)
x <- x[idx,]
x$FOR <- x$Decid + x$Conif+ x$ConifWet # forest
x$AHF <- x$Agr + x$UrbInd + x$Roads # 'alienating' human footprint
x$WET <- x$OpenWet + x$ConifWet + x$Water # wet + water
cn <- c("Open", "Water", "Agr", "UrbInd", "SoftLin", "Roads", "Decid", 
  "OpenWet", "Conif", "ConifWet")
x$HAB <- droplevels(find_max(x[,cn])$index) # drop empty levels
levels(x$HAB)[levels(x$HAB) %in% 
  c("OpenWet", "Water", "Open", "Agr", "UrbInd", "Roads")] <- "Open"
levels(x$HAB)[levels(x$HAB) %in% 
  c("Conif", "ConifWet")] <- "Conif"
x$OBS <- as.factor(x$ObserverID)
```

## Duration and distance intervals


```{r}
cts <- josm$counts[josm$counts$DetectType1 != "V",]

cts$DurDis <- paste(cts$Dur, cts$Dis)
table(cts$DurDis)

ydurdis <- Xtab(~ SiteID + DurDis + SpeciesID, cts)
```

Create count for different methodologies

```{r}
spp <- "OVEN"
y <- as.matrix(ydurdis[[spp]])
yc <- cbind(
  "3min/50m"=y[,"0-3min 0-50m"],
  "5min/50m"=y[,"0-3min 0-50m"]+y[,"3-5min 0-50m"],
  "10min/50m"=rowSums(y[,c("0-3min 0-50m", "3-5min 0-50m", "5-10min 0-50m")]),
  "3min/100m"=rowSums(y[,c("0-3min 0-50m", "0-3min 50-100m")]),
  "5min/100m"=rowSums(y[,c("0-3min 0-50m", "3-5min 0-50m",
                    "0-3min 50-100m", "3-5min 50-100m")]),
  "10min/100m"=rowSums(y[,c("0-3min 0-50m", "3-5min 0-50m", "5-10min 0-50m",
                    "0-3min 50-100m", "3-5min 50-100m", "5-10min 50-100m")]),
  "3min/Infm"=rowSums(y[,c("0-3min 0-50m", 
                    "0-3min 50-100m", "0-3min 100+m")]),
  "5min/Infm"=rowSums(y[,c("0-3min 0-50m", "3-5min 0-50m",
                    "0-3min 50-100m", "3-5min 50-100m",
                    "0-3min 100+m", "3-5min 100+m")]),
  "10min/Infm"=rowSums(y[,c("0-3min 0-50m", "3-5min 0-50m", "5-10min 0-50m",
                    "0-3min 50-100m", "3-5min 50-100m", "5-10min 50-100m",
                    "0-3min 100+m", "3-5min 100+m", "5-10min 100+m")]))
```

See how mean counts are different:

```{r}
op <- par(mar=c(8,4,2,2), las=2)
barplot(colMeans(yc), ylab="Mean count")
par(op)
```

Removal model

```{r}
Ydur <- as.matrix(Xtab(~ SiteID + Dur + SpeciesID, cts)[[spp]])
Ydur <- Ydur[idx,]
Ydur <- Ydur[rowSums(Ydur) >0,]

Ddur <- matrix(c(3, 5, 10), nrow(Ydur), 3, byrow=TRUE)

Mdur <- cmulti(Ydur | Ddur ~ TSSR + DAY , x[rownames(Ydur),], type="rem")

phi <- drop(exp(model.matrix(Mdur) %*% coef(Mdur)))
summary(phi)
```


Distance sampling

```{r}
Ydis <- as.matrix(Xtab(~ SiteID + Dis + SpeciesID, cts)[[spp]])
Ydis <- Ydis[idx,]
Ydis <- Ydis[rowSums(Ydis)>0,]
Ddis <- matrix(c(0.5, 1, 1e6), nrow(Ydis), 3, byrow=TRUE)
Mdis <- cmulti(Ydis | Ddis ~ HAB, x[rownames(Ydur),], type="dis")



tau <- drop(exp(model.matrix(Mdis) %*% coef(Mdis)))
summary(tau)
```


```{r}

# list(y = Ydur,n_sites = nrow(Ydur),
#              n_ints = ncol(Ydur),
#              n_bd =1,
#              Xd = matrix(rep(1, nrow(Ydur)),
#                          nrow = nrow(Ydur), ncol = 1),
#              tau = c(3,5,10))
sdat_com <- list(y_dist = Ydis,
                 y_dur = Ydur,
                 dist = Ddis,
                 dur = Ddur,
                 n_sites = nrow(Ydis),
                 n_dist = ncol(Ydis),
                 n_ints = ncol(Ydur)
                 )
          

library(cmdstanr)
qpad_stan <- cmdstanr::cmdstan_model("stan/RawStan_Fun.stan")
fit2 <- qpad_stan$sample(data = sdat_com, 
                         seed = 42, 
                         output_dir = "stanout", iter_warmup = 1000,
                       iter_sampling = 1000, parallel_chains = 3, chains = 3
                       )
fit2$save_object("stanout/fit2.rds")
sum2 <- fit2$summary()

```


```{r}
dat <- list(
  n_sites_dur = nrow(Ydur),
  n_sites_dist = nrow(Ydis),
  n_ints_dur = ncol(Ydur),
  n_ints_dist = ncol(Ydis),
  n_dur_parm = 3,
  n_dist_parm = 3,
  y_dur = Ydur,
  y_dist = Ydis,
  dur = Ddur,
  dist = Ddis,
  n_obs_int_dur_by_site = rep(ncol(Ydur), nrow(Ydur)),
  n_obs_int_dist_by_site = rep(ncol(Ydis), nrow(Ydis)),
  X_dur = model.matrix(~TSSR + DAY, x[rownames(Ydur),]),
  X_dist = model.matrix(~HAB, x[rownames(Ydis),])
  
)
library(cmdstanr)
cov_mod <- cmdstanr::cmdstan_model("stan/QPAD_Stan_cov.stan", 
                                     cpp_options=list(stan_threads=TRUE) )

fit_cov <- cov_mod$sample(data = dat, 
                         seed = 42, threads_per_chain = 2,
                         output_dir = "D:/!_TMP_STANFILES/NW/", iter_warmup = 1000,
                         iter_sampling = 1000, parallel_chains = 3, chains = 3
)
s <- fit_cov$summary()
s
d <- fit_cov$draws()
```

```{r}
tau_p1 <- bayesplot::mcmc_areas(d, regex_pars = "log_tau")
phi_p1 <- bayesplot::mcmc_areas(d, regex_pars = "log_phi")

summary(Mdis)
```











```{r}

curve(expr = 1 - exp(-x * phi[[1]]), from = 0,10)
curve(expr = 1 - exp(-x * sum2$median[sum2$variable=="phi"]), from = 0,10, col ='red',
      lty =2, add=T)


```


```{r}

curve(expr =1 - exp(-(x/tau[[1]])^2), from =0, 2)
curve(expr =1 - exp(-(x/sum2$median[sum2$variable=="tau"])^2), from =0, 2,
      lty=2, add=T, col='red')
curve(expr =1 - exp(-(x/sum2$q5[sum2$variable=="tau"])^2), from =0, 2,
      lty=2, add=T, col='grey')
curve(expr =1 - exp(-(x/sum2$q95[sum2$variable=="tau"])^2), from =0, 2,
      lty=2, add=T, col='grey')


```



Calculating offsets for the different methodologies (max duration and max distance)

```{r}
p_fun <- function (t, phi) {
    1 - exp(-t * phi)
}
q_fun <- function (r, tau) {
    tau^2 * (1 - exp(-r^2/tau^2))/r^2
}

## availability
p3 <-  p_fun(3, phi)
p5 <-  p_fun(5, phi)
p10 <- p_fun(10, phi)

## average detection
q50  <- q_fun(0.5, tau)
q100 <- q_fun(1, tau)
## area sampled (known)
A50 <- 0.5^2*pi
A100 <- 1^2*pi
## effective area sampled
EA <- tau^2*pi

## correction factors: C=Apq
Corr <- cbind(
  "3min/50m"=   A50 * p3  * q50,
  "5min/50m"=   A50 * p5  * q50,
  "10min/50m"=  A50 * p10 * q50,
  "3min/100m"= A100 * p3  * q100,
  "5min/100m"= A100 * p5  * q100,
  "10min/100m"=A100 * p10 * q100,
  "3min/Infm"= EA   * p3  * 1,
  "5min/Infm"= EA   * p5  * 1,
  "10min/Infm"=EA   * p10 * 1)
```



GLM with offsets: offset is the log of the correction factor: $o_i=log(C_i)=log(A_i p_i, q_i)$

```{r}
Off <- log(Corr)

```


## Different models

Let's explore models and develop:

- estimation with offset
- prediction without offset

This will be a recipe book that you can use. We just did the Poisson GLM, but repeat it here with the 5 min 100 m counts (you can change this of course). We'll keep counts, offsets, and the fixed effects the same

```{r}
x <- x[order(x$Decid),] # order according to Decid
y <- yc[rownames(x),"5min/100m"]
o <- Off[rownames(x),"5min/100m"]
X <- model.matrix(~ Decid, x)
```


### Poisson GLM

```{r}
## model fit
MP <- glm(y ~ Decid, data=x, family=poisson, offset=o)

## predicting D
DP <- drop(exp(X %*% coef(MP)))
```



```{r}
# data{
#   // Data dimensions
#   int<lower=1> n_sites;     // Number of surveys
#   int<lower=1> n_ints;      // Number of censored detection intervals
#   int<lower=1> n_dist;      // Number of distance rings
#   int<lower=1> n_pred;      // Number of covariates
#   int<lower=1> n_dat;       // number of data points for glm
#   int<lower=0> y_dur[n_sites,n_ints];  // Counts by survey and observation interval
#   int<lower=0> y_dist[n_sites,n_ints];  // Counts by survey and observation interval
# 
# // Covariates
# // int<lower=0> n_bd;        // Number of detection fixed effects
# // matrix[n_sites,n_bd] Xd;  // Explanatory variables for detection
#   // Data
#   matrix[n_sites,n_ints] dur;
#   matrix[n_sites,n_ints] dist;
#   // real<lower=0> tau[n_ints];       // Endpoints of observation intervals
# 
#   int y_count[n_dat];  // Counts for glm
#   real time[n_dat]; // Time intervals for each points
#   real glm_dist[n_dat]; // Max bands for glm points
#   matrix[n_dat, n_pred] Xd; // Glm covariates
# 
# 
# }

Ydur_st <- Ydur[rowSums(Ydur) >0,]
Ddur_st <- matrix(c(3, 5, 10), nrow(Ydur_st), 3, byrow=TRUE)
Ydis_st <- Ydis[rowSums(Ydis)>0,]
Ddist_st <- matrix(c(0.5, 1, 1e6), nrow(Ydis_st), 3, byrow=TRUE)

stan_data_glm <- 
  list(
    n_sites = nrow(Ydur_st),
    n_ints = ncol(Ddur_st),
    n_dist = ncol(Ddist_st),
    n_pred = ncol(X),
    n_dat = nrow(X),
    y_dist = Ydis_st,
    y_dur = Ydur_st,
    dist = Ddis_st,
    dur = Ddur_st,
    y_count = y,
    time = rep(5, times =length(y)),
    glm_dist = rep(1, times = length(y)),
    Xd = X
  )
  
library(cmdstanr)
qpad_stan_glm <- cmdstanr::cmdstan_model("stan/QPAD_GLM.stan")
glm_fit_qpad <- qpad_stan_glm$sample(data = stan_data_glm, 
                         seed = 42, 
                         output_dir = "stanout", iter_warmup = 1000,
                       iter_sampling = 1000, parallel_chains = 3, chains = 3
                       )

sum_glm <- glm_fit_qpad$summary()


```



### Negative Binomial GLM


```{r regr-dist2}
MNB <- glm.nb(y ~ Decid + offset(o), data=x)

DNB <- drop(exp(X %*% coef(MNB)))
```

### Zero inflated Poisson

```{r regr-dist3}
MZIP <- zeroinfl(y ~ Decid | 1, x, dist="poisson", offset=o)

DZIP <- drop((1-plogis(coef(MZIP, "zero"))) * exp(X %*% coef(MZIP, "count")))
```


```{r}
MZINB <- zeroinfl(y ~ Decid | 1, x, dist="negbin", offset=o)

DZINB <- drop((1-plogis(coef(MZINB, "zero"))) * exp(X %*% coef(MZINB, "count")))
```

### Occupancy

```{r}
y01 <- ifelse(y > 0, 1, 0)
MB <- glm(y01 ~ Decid, data=x, family=binomial("cloglog"), offset=o)

## note: we are using exp instead of cloglog inverse link
DB <- drop(exp(X %*% coef(MB)))
```


## Poisson GAM

```{r}
MPG <- mgcv::gam(y ~ s(Decid), data=x, family=poisson, offset=o)

DPG <- predict(MPG, newdata=x, type="response")
```


## Poisson-Lognormal GLMM in lme4


```{r}
MPLN <- glmer(y ~ Decid + (1 | SiteID), data=x, family=poisson, offset=o)

DPLN <- drop(exp(X %*% fixef(MPLN)))
```



### Regression trees

```{r}
library(gbm)
library(dismo)
library(ggplot2)

xi <- data.frame(y=y,
  x[,c("Open", "Water", "Agr", "UrbInd", "SoftLin", "Roads", 
  "Decid", "OpenWet", "Conif", "ConifWet", "FOR", "AHF", "WET")])
```

```{r eval=FALSE}
# this does k-fold cross validation
brt <- gbm.step(xi,
        gbm.y = 1,
        gbm.x = 2:ncol(xi),
        offset = o,
        family = "poisson",
        tree.complexity = 3,
        learning.rate = 0.01,
        bag.fraction = 0.5)
DBRT <- predict.gbm(brt, xi, brt$n.trees, type="response")
```

```{r results="hide"}
brt <- gbm(y ~ . + offset(o),
        data=xi,
        n.trees = 1000,
        interaction.depth = 3,
        shrinkage = 0.001,
        bag.fraction = 0.5,
        distribution = "poisson",
        var.monotone = NULL,
        keep.data = FALSE,
        verbose = FALSE,
        n.cores = 1)
DBRT <- predict.gbm(brt, xi, brt$n.trees, type="response")
```

Variable importance

```{r}
rel_inf <- function(res) {
    rel.inf <- relative.influence(res, res$n.trees)
    rel.inf[rel.inf < 0] <- 0
    i <- order(-rel.inf)
    rel.inf <- 100 * rel.inf/sum(rel.inf)
    out <- data.frame(var = res$var.names[i], rel.inf = rel.inf[i])
    attr(out, "n.trees") <- res$n.trees
    out
}
rel_inf(brt)
```

Marginal effects plots

```{r}
.plot_fun <- function(i, res, u) {
    j <- as.character(u$var[i])
    x <- plot.gbm(res, j,
        n.trees = res$n.trees,
        return.grid=TRUE,
        type="response",
        ylab=paste(res$rof_settings$spp, "density (males/ha)"),
        xlab=paste0(j, " (", round(u$rel.inf[i], 2), "%)"))
    colnames(x) <- c("x", "y")
    x$var <- paste0(j, " (", round(u$rel.inf[i], 2), "%)")
    attr(x, "out.attrs") <- NULL
    x
}
plot_fun <- function(res) {
    u <- rel_inf(res)
    xx <- do.call(rbind, lapply(1:12, .plot_fun, res, u))
    xx$var <- factor(xx$var, unique(xx$var))
    p <- ggplot(xx, aes(x=x, y=y)) +
        geom_line() +
        facet_wrap(vars(var), scales="free_x") +
        ylab(paste(res$rof_settings$spp, "density (males/ha)")) +
        xlab("Predictor values") +
        theme_minimal()
    p
}
print(plot_fun(brt))
```

### Compare results

```{r}
AIC(MP, MNB, MZIP, MZINB, MPG, MPLN)
bbmle::AICctab(MP, MNB, MZIP, MZINB, MPG, MPLN)

mean(DP)
mean(DB)
mean(DNB)
mean(DZIP)
mean(DZINB)
mean(DPG)
mean(DPLN)
mean(DBRT)
```


## Propagating error

Use the Poisson GLM to show how to propagate uncertainty from the removal and distance model into the GLM


```{r}
library(MASS)
B <- 100

Xdur <- model.matrix(Mdur)
(cfDur <- coef(Mdur))
(vcvDur <- vcov(Mdur))

cfBDur <- mvrnorm(B, cfDur, vcvDur)
head(cfBDur)

phiB <- apply(cfBDur, 1, function(z) {
  drop(exp(Xdur %*% z))
})

p5B <-  apply(phiB, 2, function(z) p_fun(5, z))
```

```{r}
Xdis <- model.matrix(Mdis)
(cfDis <- coef(Mdis))
(vcvDis <- vcov(Mdis))

cfBDis <- mvrnorm(B, cfDis, vcvDis)
tauB <- apply(cfBDis, 1, function(z) {
  drop(exp(Xdis %*% z))
})

q100B <- apply(tauB, 2, function(z) q_fun(1, z))

oB <- log(A100 * p5B * q100B)
rethinking::dens(oB[1,])
```

```{r}
COEF1 <- COEF2 <-  matrix(0, B, 2)
for (i in 1:B) {
  ii <- sample.int(nrow(x), nrow(x), replace=TRUE)
  ## no error propagation
  m1 <- glm(y[ii] ~ Decid, data=x[ii,], family=poisson, offset=o[ii])
  COEF1[i,] <- coef(m1)
  ## error propagation
  m2 <- glm(y[ii] ~ Decid, data=x[ii,], family=poisson, offset=oB[ii,i])
  COEF2[i,] <- coef(m2)
}
apply(COEF1, 2, sd)
apply(COEF1, 2, mean)
apply(COEF1, 2, quantile, prob = c(0.945))
apply(COEF2, 2, quantile, prob = c(0.945))
apply(COEF2, 2, sd)
```

```{r}
Xpred2 <- Xpred[c(1,5,10,15,20),]
pr1 <- t(apply(exp(Xpred2 %*% t(COEF1)), 1, 
               quantile, c(0.05, 0.95)))
pr2 <- t(apply(exp(Xpred2 %*% t(COEF2)), 1, 
               quantile, c(0.05, 0.95)))
data.frame(none=pr1, prop=pr2)
```

## Model validation

Using offsets can have some side effects. Let's inspect the performance of a null and fixed effect GLM.

When we look at observed vs. predicted based goodness of fit tests or AUC type metrics, we need $\lambda=DApq=D e^{offset}$

```{r}
M0 <- glm(y ~ 1, data=x, family=poisson, offset=o)
M1 <- glm(y ~ Decid, data=x, family=poisson, offset=o)

lam0 <- drop(exp(X[,1,drop=FALSE] %*% coef(M0))) * exp(o)
lam1 <- drop(exp(X %*% coef(M1))) * exp(o)
```

Boxplot: when we have different methods mixed together the expected counts can vary greatly (not the case here, because we fixed methodology). Thus offsets can drive the goodness of fit metric.

```{r}
boxplot(lam0 ~ y)
boxplot(lam1 ~ y)
df_ <- tibble::tibble(y, lam0, lam1)

ggplot(df_, aes(y, lam0, group = y)) + geom_violin()
ggplot(df_, aes(y, lam0, group = y)) + geom_point() +
  geom_smooth(group = NULL)
ggplot(df_, aes(y, lam1, group = y)) + geom_violin()
```

```{r}
simple_roc <- function(labels, scores){
    Labels <- labels[order(scores, decreasing=TRUE)]
    data.frame(
        TPR=cumsum(Labels)/sum(Labels),
        FPR=cumsum(!Labels)/sum(!Labels),
        Labels=Labels)
}
simple_auc <- function(ROC) {
    ROC$inv_spec <- 1-ROC$FPR
    dx <- diff(ROC$inv_spec)
    sum(dx * ROC$TPR[-1]) / sum(dx)
}
roc0 <- simple_roc(ifelse(y>0,1,0), lam0)
roc1 <- simple_roc(ifelse(y>0,1,0), lam1)
(auc0 <- simple_auc(roc0))
(auc1 <- simple_auc(roc1))

plot(roc0[,2:1], type="l")
lines(roc1[,2:1], col=2)
abline(0,1,lty=2)
legend("topleft", bty="n", lty=1, col=c(1,2), legend=c("Null", "Decid"))
```





